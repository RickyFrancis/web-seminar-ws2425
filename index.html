<!DOCTYPE html>
<html>
  <head>
    <title>Seminar Web Engineering in Winter Semester 2024/25</title>
    <link rel="stylesheet" type="text/css" href="main.css" />
    <link
      href="http://fonts.googleapis.com/css?family=Source+Serif+Pro:400,600,700"
      rel="stylesheet"
      type="text/css"
    />
    <link
      href="http://fonts.googleapis.com/css?family=Inconsolata:400,700"
      rel="stylesheet"
      type="text/css"
    />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  </head>
  <body>
    <header>
      <h2>Seminar Web Engineering in Winter Semester 2024/25</h2>
      <h1>Prompt Engineering with Multimodal Data</h1>
      <h2 class="author">Ricky Francis Rozario, Syed Bilal Haider</h2>
      <h2 class="supervisor">Advisor: Dr. Sheeba Samuel</h2>
      <h3 class="affiliation">
        Technische Universität Chemnitz<br />
        Chemnitz, Germany
      </h3>
    </header>

    <section>
      <h2>2. Demonstration</h2>
      <h3>
        2.1. Identifying Discrepancies: Analyzing Grocery Receipts vs. Product
        Images
      </h3>

      <p>
        To showcase the multimodal capabilities of a Large Language Model (LLM)
        [53], we designed a demonstration analyzing grocery items using two
        modalities: product images and the purchase receipt. This illustrates
        how LLMs identify discrepancies between visual and textual data.
      </p>
      <p>First, the LLM is given an image of grocery items with the prompt:</p>
      <p>
        <i>
          “Analyze the attached image. List all the items visible in the image.
          Some of the items might not be fully visible; include those as well.”
        </i>
      </p>
      <figure>
        <img
          src="images/grocery-items-picture.png"
          alt="grocery-items-picture"
        />
        <figcaption>
          <strong>Figure 50</strong>: Picture of grocery items used in the
          demonstration
          <!-- <a href="#r13">[13]</a>. -->
        </figcaption>
      </figure>
      <p>
        The LLM processes the textual prompt by tokenizing
        <a href="#r50">[50]</a> and generating vector embeddings
        <a href="#r50">[50]</a>, while simultaneously preprocessing the image
        <a href="#r51">[51]</a> to extract visual features and create embeddings
        <a href="#r51">[51]</a> . These embeddings are unified into a single
        vector representation.
      </p>
      <p>Next, the LLM is provided with a receipt image and prompted:</p>
      <p>
        <i> “List all the items visible in this purchase receipt.” </i>
      </p>
      <figure>
        <img
          src="images/grocery-items-receipt.png"
          alt="grocery-items-receipt"
        />
        <figcaption>
          <strong>Figure 51</strong>: Picture of purchase receipt of the grocery
          items used in the demonstration
          <!-- <a href="#r13">[13]</a>. -->
        </figcaption>
      </figure>
      <p>
        The LLM follows a similar process, tokenizing the prompt and generating
        embeddings for the textual and visual data, which are then combined into
        another unified vector. Finally, the LLM compares the two vectors using
        cosine similarity analysis <a href="#r52">[52]</a> to identify
        discrepancies between the grocery image and the receipt. It produces two
        lists:
      </p>
      <ul>
        <li>
          <i>
            Items in the receipt but not in the picture: ja! H-Milch 1,5%
            (milk).</i
          >
        </li>
        <li>
          <i>
            Items in the picture but not on the receipt: Em-eukal Honey
            Lozenges.</i
          >
        </li>
      </ul>
      <p>
        This demonstration highlights the LLM’s ability to integrate and analyze
        multimodal data, bridging the gap between visual and textual
        information. It has practical applications, such as identifying errors
        or potential fraud in retail inventory management.
      </p>
    </section>
    <section>
      <h3>2.2. Evaluating a Medical Report Using an LLM</h3>
      <p>
        Large Language Models (LLMs) can effectively analyze multimodal data,
        such as in healthcare, where diverse data types are common. This
        demonstration tasks an LLM with evaluating a patient’s ultrasonography
        report to summarize their thyroid condition. The report includes textual
        findings and graphical sonographic images.
      </p>
      <p>The LLM is provided the report file and the prompt:</p>
      <p>
        <i>
          “Attached is an ultrasonography report of a patient. The report
          contains both textual and graphical data of the patient’s neck. The
          main goal of this report was to find the condition of the thyroid in
          the patient’s neck region. Analyzing both textual data and graphical
          data, create a comprehensive report that will portray the condition of
          the patient and point out any abnormalities.”
        </i>
      </p>
      <figure>
        <img src="images/USG_of_Neck.jpg" alt="USG_of_Neck" />
        <figcaption>
          <strong>Figure 52</strong>: Ultrasonography report of a patient’s neck
          <!-- <a href="#r13">[13]</a>. -->
        </figcaption>
      </figure>
      <p>
        The LLM first analyzes the textual data, identifying medical
        terminology, diagnostic conclusions, and measurements related to the
        thyroid. Simultaneously, it interprets the graphical data, detecting
        abnormalities like nodules or cysts. By integrating insights from both
        modalities, the LLM generates a detailed summary of the patient’s
        thyroid condition, which aids in diagnosis and clinical decision-making.
        We have used Llava multimodal model <a href="#r53">[53]</a> for this
        task along with OpenWebUi <a href="#r54">[54]</a>
        for the interface
      </p>
      <figure>
        <img src="images/llava-response.png" alt="USG_of_Neck" />
        <figcaption>
          <strong>Figure 53</strong>: Response generated by the Llava multimodal
          model
          <!-- <a href="#r13">[13]</a>. -->
        </figcaption>
      </figure>
      <p>
        This demonstration highlights the LLM’s potential in healthcare,
        offering efficient, multimodal analysis to support medical
        professionals.
      </p>
    </section>
    <section>
      <h2>3. Examples: Best Multimodal Prompt Engineering Practices</h2>
      <h3>3.1. Generating session notes from audio</h3>
      <p>
        Effective prompt design is crucial for obtaining desired results from an
        LLM. Below, we compare suboptimal and improved approaches to analyzing
        an audio recording and its transcript to generate session notes.
      </p>
      <p>
        <b>Suboptimal Prompt:</b>
      </p>
      <p>
        <em>
          “Listen to the attached audio and review the transcript to generate
          session notes.”
        </em>
      </p>
      <p>
        <b>Analysis:</b>
        This prompt is vague and lacks context. It fails to define what “session
        notes” entail, omits the LLM’s role, and provides no structure or
        guidelines for the output. This ambiguity may lead to incomplete,
        disorganized, or unclear results.
      </p>
      <p>
        <b>Improved Prompt:</b>
      </p>
      <p>
        <b>Role Assignment:</b> “You are an expert in analyzing lectures and
        creating structured session notes.” <br />
        <b>Context:</b> “Analyze the attached audio and transcript of a lecture
        on C# methods.”<br />
        <b>Instructions:</b>
        <em
          >"Summarize the lecture’s key learnings to ensure readability and
          clarity. Include questions asked and the lecturer’s answers.
          Incorporate shared tips, best practices, or personal advice."</em
        ><br />
        <b>Constraints:</b>
        <em
          >“No word count limits; ensure the notes are thorough and
          comprehensive.”</em
        >
      </p>
      <p>
        <b>Analysis:</b>
        This prompt assigns a role, provides context, and clearly outlines the
        task with specific instructions. It ensures the LLM understands
        expectations and delivers a detailed, well-structured output.
      </p>
    </section>
    <section>
      <h3>3.2 Estimating Oil Usage Duration</h3>
      <p>
        Prompt engineering plays a vital role in achieving clear, accurate, and
        contextually relevant responses from Large Language Models (LLMs).
        Below, we compare a suboptimal prompt with an improved version to
        demonstrate how clarity and specificity in prompt design significantly
        impact the quality of the response.
      </p>
      <p>
        <b>Suboptimal Prompt:</b>
      </p>
      <p>
        <em> “How long will these oil last in my restaurant?” </em>
      </p>
      <figure>
        <img src="images/oil-bottles.png" alt="oil bottles" />
        <figcaption>
          <strong>Figure 53</strong>: Picture of oil bottles that was submitted
          to the MLLM
          <!-- <a href="#r13">[13]</a>. -->
        </figcaption>
      </figure>
      <p>
        <b>LLM Response:</b>
      </p>
      <p>
        <em> “More details are needed to perform an estimation.” </em>
      </p>
      <p>
        <b>Analysis:</b>
        This prompt is too vague. It does not specify the quantity of oil, the
        usage rate, or provide relevant details about the bottles. Without
        context, the LLM cannot reason effectively or provide a meaningful
        answer.
      </p>
      <p>
        <b>Improved Prompt:</b>
      </p>
      <p>
        <em>
          “Attached are five-liter cooking oil bottles. Considering 1 liter of
          oil is used in my restaurant per day, how long will the oils from the
          picture last?”
        </em>
      </p>
      <p>
        <b>LLM Response:</b>
      </p>
      <p>
        <em>
          “The image shows three bottles of 5-liter cooking oil each. The three
          5-liter bottles total 15 liters. At 1 liter per day, the oil will last
          15 days.”
        </em>
      </p>

      <p>
        <b>Analysis:</b>
        This prompt assigns a role, provides context, and clearly outlines the
        task with specific instructions. It ensures the LLM understands
        expectations and delivers a detailed, well-structured output.
      </p>
      <p>
        <b> Key Takeaways for Efficient Prompt Design:</b>
      </p>
      <ul>
        <li>
          Provide Context: Include background details, like object
          characteristics or task constraints.
        </li>
        <li>Specify Details: Use quantitative data for clarity.</li>
        <li>
          Reference Attachments Clearly: Highlight the role of images or other
          inputs.
        </li>
        <li>Structure Queries: Break tasks into clear, manageable steps.</li>
      </ul>
    </section>
    <section>
      <h2>4. Leveraging Multimodal LLMs</h2>
      <h3>
        4.1. Automate TypeScript Interface Creation from JSON API Responses
      </h3>
      <p>
        Multimodal LLMs can be used to offload tedious tasks from humans to AI.
        One such task is generating typescript interfaces from API response.
      </p>
      <p>JSON file for example:</p>
      <figure>
        <img src="images/api-response-json.png" alt="api-response-json" />
        <figcaption>
          <strong>Figure 51</strong>: JSON file representing an API response
          <!-- <a href="#r13">[13]</a>. -->
        </figcaption>
      </figure>
      <p>
        By using an LLM, this task can be automated. The JSON file is provided
        as input alongside a textual prompt such as: <br />
        <em>
          "You are a TypeScript expert. Analyze the attached JSON file, which
          represents an API response. Generate a TypeScript interface that
          correctly reflects the structure and types of the JSON data. Ensure
          proper naming conventions and type annotations."</em
        >
      </p>
      <p>
        The multimodal LLM processes this input by analyzing the JSON structure
        to infer the types of the key-value pairs. The output generated by the
        LLM for the given example might look like this:
      </p>
      <figure>
        <img src="images/ts-interface.png" alt="ts-interface" />
        <figcaption>
          <strong>Figure 51</strong>: TypeScript interface generated by the AI
        </figcaption>
      </figure>
    </section>
    <section>
      <h3>4.2. Smart Home Assistants</h3>
      <p>
        The Queensland-based smart assistant Cora
        <a href="#r55">[55]</a> demonstrates the power of Multimodal Large
        Language Models (MLLMs) by processing audio, visual, and text data. Cora
        enhances security through real-time camera monitoring, intrusion
        detection, and alerts. It also supports voice commands to control
        appliances and check updates, creating a secure and interactive smart
        home environment.
      </p>
    </section>
    <section>
      <h3>4.3. Security Surveillance</h3>
      <p>
        MLLMs and Vision Transformers (ViTs) are pivotal in visual-based malware
        detection <a href="#r56">[56]</a>. ViTs achieve a 97.11% F1-score in
        classifying 25 malware types, showcasing accuracy and specialization.
        MLLMs, though versatile, are less precise, highlighting the trade-off
        between adaptability and performance in cybersecurity applications.
      </p>
    </section>
    <section>
      <h3>4.4. Smart Healthcare Assistants</h3>
      <p>
        Laguna Health’s AI <a href="#r57">[57]</a>, Laguna Insight, transforms
        patient care by providing summaries before calls, reducing admin tasks.
        Real-time insights during calls enhance empathy and communication,
        boosting efficiency and trust. This underscores AI’s growing role in
        healthcare.
      </p>
    </section>
    <section>
      <h2>5. Multimodal Data Combinations</h2>

      <p>
        Blending data types like text, audio, images, and tables unlocks new
        possibilities for advanced AI applications. Models such as GPT-4,
        Google’s PaLM-E, and Meta’s ImageBind use these combinations to solve
        complex tasks, each with unique strengths and challenges.
      </p>

      <figure>
        <img src="images/data-combinations.jpg" alt="data-combinations" />
        <figcaption>
          <strong>Figure 51</strong>: Multimodal data combinations
        </figcaption>
      </figure>

      <p>
        <b>Text + Audio:</b> This pairing is ideal for analyzing spoken
        language, sound patterns, or music. Systems like OpenAI’s Whisper handle
        transcription, sentiment analysis, and audio-based search. For example,
        a customer service call can be summarized and analyzed for emotion and
        key points. Challenges include dealing with accents, background noise,
        and syncing audio to text. Common uses include virtual assistants,
        podcast indexing, and meeting notes.
      </p>

      <p>
        <b>Text + Image:</b> Combining text and images powers applications like
        image captioning, object detection, and creative tasks (e.g., generating
        art). In e-commerce, product images with descriptions can create
        marketing content or identify visual issues. Challenges include handling
        vague visuals, scaling to diverse images, and ethical concerns like
        fairness in datasets. Applications range from medical imaging
        diagnostics to education.
      </p>
      <p>
        <b>Text + Tabular Data:</b> Text and tables excel in fields like finance
        and logistics. Models can summarize trends or detect anomalies in
        structured datasets. Challenges include interpreting complex tables,
        managing large datasets, and domain-specific accuracy. Use cases include
        report generation, predictive analytics, and personalized
        recommendations.
      </p>
    </section>
    <section>
      <h2>6. Potential Challenges</h2>

      <p>
        <b>Data quality and bias <a href="#r58">[58]</a>:</b> MLLMs rely on
        large datasets, which often include biases and inaccuracies. These can
        reinforce harmful stereotypes or spread misinformation. Ensuring
        diverse, high-quality training data is key to minimizing these risks and
        promoting fairness.
      </p>

      <p>
        <b>High computational cost <a href="#r59">[59]</a>:</b> Training and
        running MLLMs require significant energy and resources, making them
        expensive and environmentally taxing. This creates barriers for smaller
        organizations and raises concerns about sustainability. Efficient
        algorithms and renewable energy can help address these issues.
      </p>
      <p>
        <b>Interpretability <a href="#r60">[60]</a>:</b> MLLMs have complex
        architectures, making it hard to understand how they generate results.
        This lack of transparency lowers trust, especially in critical areas
        like healthcare or law. Explainability tools and traceable decision
        paths are essential for building confidence.
      </p>
      <p>
        <b>Multimodal alignment <a href="#r61">[61]</a>:</b> Combining text,
        images, and audio is challenging. Poor alignment can cause errors, such
        as mismatching visual and textual data. Improved techniques are
        necessary to ensure accurate integration of diverse inputs.
      </p>
      <p>
        <b>Ethical concerns <a href="#r62">[62]</a>:</b> MLLMs may produce
        harmful or inappropriate content, raising issues in public or sensitive
        applications. Strong safeguards, like content moderation and ethical
        guidelines, are crucial to prevent misuse.
      </p>
      <p>
        <b>Scalability <a href="#r63">[63]</a>:</b> As MLLMs grow in complexity,
        their deployment becomes costly and resource-intensive. Scalable
        solutions are needed to make them accessible for wider use without
        compromising effectiveness or increasing costs.
      </p>
    </section>
    <section>
      <h2>7. Future Potential</h2>

      <p>
        <b>Better Human-AI Interaction <a href="#r64">[64]</a>:</b> Multimodal
        data allows AI to process complex inputs like speech, gestures, and
        images. This improves human-AI interaction, making virtual assistants
        more intuitive. For example, in education, healthcare, and customer
        support, it enables natural and seamless communication, improving
        overall user experiences.
      </p>
      <p>
        <b>Improved Multimodal Reasoning <a href="#r65">[65]</a>:</b> AI can
        combine and analyze text, images, and audio to provide accurate and
        relevant results. This helps with tasks like answering visual questions
        or retrieving cross-format data, making the outputs more precise and
        context-aware.
      </p>
      <p>
        <b>Personalized Content <a href="#r66">[66]</a>:</b> AI synthesizes data
        from various sources to create tailored outputs. For example, in
        marketing, AI can generate personalized multimedia ads based on a user’s
        preferences, demographics, and browsing habits, increasing engagement
        and effectiveness. This also benefits entertainment and education by
        delivering customized experiences.
      </p>
      <p>
        <b>Efficient Multimodal Fusion <a href="#r67">[67]</a>:</b> Advanced
        techniques optimize the integration of data from different modalities,
        reducing computational costs. This efficiency is crucial for
        resource-limited scenarios, such as deploying AI on mobile devices or in
        edge computing.
      </p>
      <p>
        <b>Smarter Applications <a href="#r68">[68]</a>:</b> Multimodal prompts
        enhance AI in robotics, speech recognition, and video analysis. Robots,
        for instance, can better understand visual scenes while interpreting
        spoken instructions, enabling smarter, more context-aware actions.
      </p>
      <p>
        <b>Improved Security <a href="#r69">[69]</a>:</b> Thoughtfully designed
        prompts help protect AI systems from attacks, improving reliability in
        sensitive areas like autonomous vehicles, healthcare, and financial
        systems. This ensures AI systems remain trustworthy and robust.
      </p>
    </section>
    <section>
      <h2>8. Conclusion</h2>

      <p>
        Multimodal AI is revolutionizing how we interact with technology by
        combining data like text, images, and audio, making AI more natural and
        intuitive. As more data modalities are integrated into AI systems,
        prompt engineering is becoming increasingly important to unlock their
        full potential. Prompt engineering enables users to craft precise
        instructions, guiding AI to produce more accurate, context-aware, and
        meaningful responses. In the future, as AI continues to evolve and
        tackle more complex tasks, effective prompt engineering will be
        essential for maximizing its capabilities, ensuring user-friendly
        interactions, and driving innovation across fields like healthcare,
        education, and entertainment.
      </p>
    </section>
    <section class="references">
      <h2>6. References</h2>
      <p class="reference" id="r50">
        [50] T. Mikolov, K. Chen, G. Corrado, and J. Dean, "From Word Vectors to
        Multimodal Embeddings: Techniques and Applications," arXiv preprint
        arXiv:2411.05036, Nov. 2024. [Online]. Available:
        <a href="https://arxiv.org/abs/2411.05036"
          >https://arxiv.org/abs/2411.05036</a
        >
      </p>
      <p class="reference" id="r51">
        [51] X. Wang, A. Patel, and J. Lin, "What Makes for Good Visual
        Tokenizers for Large Language Models?" arXiv preprint arXiv:2305.12223,
        May 2023. [Online]. Available:
        <a href="https://arxiv.org/abs/2305.12223"
          >https://arxiv.org/abs/2305.12223</a
        >
      </p>
      <p class="reference" id="r52">
        [52] S. Huang, L. Dong, W. Wang, et al., "Language is not all you need:
        Aligning perception with language models," arXiv preprint,
        arXiv:2302.14045, Feb. 27, 2023. [Online]. Available:
        <a href="https://arxiv.org/pdf/2302.14045"
          >https://arxiv.org/pdf/2302.14045</a
        >
      </p>
      <p class="reference" id="r53">
        [53] H. Liu, C. Li, Q. Wu, and Y. J. Lee, "Visual Instruction Tuning,"
        arXiv preprint arXiv:2304.08485, Dec. 2023. [Online]. Available:
        <a href="https://arxiv.org/abs/2304.08485"
          >https://arxiv.org/abs/2304.08485</a
        >
      </p>
      <p class="reference" id="r54">
        [54] "OpenWebUI Whitepaper," OpenWebUI Whitepaper, [Online]. Available:
        <a href="https://openwebui.com/assets/files/whitepaper.pdf"
          >https://openwebui.com/assets/files/whitepaper.pdf</a
        >[Accessed: Jan. 20, 2025]
      </p>
      <p class="reference" id="r55">
        [55] "Next-generation Aussie housing: AI assistants and in-home medical
        care," PropTech Pro, [Online]. Available:
        <a
          href="https://proptechpro.com.au/next-generation-aussie-housing-ai-assistants-and-in-home-medical-care/"
          >https://proptechpro.com.au/next-generation-aussie-housing-ai-assistants-and-in-home-medical-care/</a
        >
        [Accessed: Dec. 9, 2024]
      </p>
      <p class="reference" id="r56">
        [56] "Evaluating the efficacy of prompt-engineered large multimodal
        models versus fine-tuned vision transformers in image-based security
        applications," [Online]. Available:
        <a href="https://arxiv.org/abs/2403.17787"
          >https://arxiv.org/abs/2403.17787</a
        >
        [Accessed: Dec. 9, 2024]
      </p>

      <p class="reference" id="r57">
        [57] "Laguna Insight," Time, [Online]. Available:
        <a href="https://time.com/7094937/laguna-insight/"
          >https://time.com/7094937/laguna-insight/</a
        >
        [Accessed: Dec. 9, 2024]
      </p>
      <p class="reference" id="r58">
        [58] J. Wang et al., "A comprehensive review of multimodal large
        language models: Performance and challenges across different tasks,"
        arXiv preprint, arXiv:2408.01319, 2024. doi:
        <a href="https://doi.org/10.48550/arXiv.2408.01319"
          >https://doi.org/10.48550/arXiv.2408.01319</a
        >
      </p>
      <p class="reference" id="r59">
        [59] D. Zhang, Y. Yu, J. Dong, C. Li, D. Su, C. Chu, and D. Yu,
        "MM-LLMs: Recent advances in multimodal large language models," arXiv
        preprint, arXiv:2401.13601, 2024. [Online]. Available:
        <a href="https://arxiv.org/abs/2401.13601"
          >https://arxiv.org/abs/2401.13601</a
        >
      </p>
      <p class="reference" id="r60">
        [60] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen, "A
        survey on multimodal large language models," National Science Review,
        vol. nwae403, 2024. doi: 10.1093/nsr/nwae403.
        <a href="https://doi.org/10.1093/nsr/nwae403"
          >https://doi.org/10.1093/nsr/nwae403</a
        >
      </p>
      <p class="reference" id="r61">
        [61] C. X. Liang et al., "A comprehensive survey and guide to multimodal
        large language models in vision-language tasks," arXiv preprint,
        arXiv:2411.06284, 2024.
        <a href="https://doi.org/10.48550/arXiv.2411.06284"
          >https://doi.org/10.48550/arXiv.2411.06284</a
        >
      </p>
      <p class="reference" id="r62">
        [62] R. AlSaad, A. Abd-alrazaq, S. Boughorbel, A. Ahmed, M.-A. Renault,
        R. Damseh, and J. Sheikh, "Multimodal large language models in health
        care: Applications, challenges, and future outlook," Journal of Medical
        Internet Research, vol. 26, p. e59505, 2024.
        <a href="https://doi.org/10.2196/59505"
          >https://doi.org/10.2196/59505</a
        >
      </p>
      <p class="reference" id="r63">
        [63] D. Huang, C. Yan, Q. Li, and X. Peng, "From large language models
        to large multimodal models: A literature review," Applied Sciences, vol.
        14, no. 12, p. 5068, 2024.
        <a href="https://doi.org/10.3390/app14125068"
          >https://doi.org/10.3390/app14125068</a
        >
      </p>
      <p class="reference" id="r64">
        [64] W. Cain, "Prompting change: Exploring prompt engineering in large
        language model AI and its potential to transform education," TechTrends,
        vol. 68, pp. 47–57, 2024.
      </p>
      <p class="reference" id="r65">
        [65] J. He, X. Wang, S. Liu, G. Wu, C. Silva, and H. Qu, "POEM:
        Interactive prompt optimization for enhancing multimodal reasoning of
        large language models," arXiv preprint, arXiv:2406.03843, 2024.
        <a href="https://doi.org/10.48550/arXiv.2406.03843"
          >https://doi.org/10.48550/arXiv.2406.03843</a
        >
      </p>
      <p class="reference" id="r66">
        [66] A. Bozkurt, "Tell me your prompts and I will make them true: The
        alchemy of prompt engineering and generative AI," Open Praxis, vol. 16,
        no. 2, pp. 111–118, 2024.
      </p>
      <p class="reference" id="r67">
        [67] R. Jiang, L. Liu, and C. Chen, "MoPE: Parameter-efficient and
        scalable multimodal fusion via mixture of prompt experts," arXiv
        preprint, arXiv:2403.10568, 2024.
        <a href="https://doi.org/10.48550/arXiv.2403.10568"
          >https://doi.org/10.48550/arXiv.2403.10568</a
        >
      </p>
      <p class="reference" id="r68">
        [68] "The future of prompt engineering: Trends and predictions for AI
        development," Boston Institute of Analytics, 2024.
      </p>
      <p class="reference" id="r69">
        [69] B. Chen, Z. Zhang, N. Langrené, and S. Zhu, "Unleashing the
        potential of prompt engineering in large language models: A
        comprehensive review," arXiv preprint, arXiv:2310.14735, 2024.
        <a href="https://doi.org/10.48550/arXiv.2310.14735"
          >https://doi.org/10.48550/arXiv.2310.14735</a
        >
      </p>
    </section>
  </body>
</html>
